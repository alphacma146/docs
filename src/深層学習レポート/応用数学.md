---
puppeteer:
    displayHeaderFooter: true
    headerTemplate: '<div style="font-size: 12px; margin-left: 1cm;"></div>'
    footerTemplate: '<div style="font-size: 12px; margin-left: 19cm;"><span class="pageNumber"></span>/<span class="totalPages"></span></div>'
---

# 応用数学　レポート

---

<div style="text-align: right;">

2024 年 06 月 06 日 新規作成

</div>

---

<!-- code_chunk_output -->

- [応用数学　レポート](#応用数学レポート)
  - [第一章 線形代数](#第一章-線形代数)
    - [1.1. スカラーとベクトル](#11-スカラーとベクトル)
    - [1.2. 行列](#12-行列)
    - [1.3. 逆行列](#13-逆行列)
    - [1.4. 行列式](#14-行列式)
    - [1.5. 行基本変形](#15-行基本変形)
    - [1.6. 固有値分解](#16-固有値分解)
    - [1.7. 特異値分解](#17-特異値分解)
  - [第二章 確率・統計](#第二章-確率統計)
    - [2.1. 確率](#21-確率)
    - [2.2. 条件付確率](#22-条件付確率)
    - [2.3. 確率変数と確率分布](#23-確率変数と確率分布)
    - [2.4. 確率変数の期待値と分散](#24-確率変数の期待値と分散)
    - [2.5. 共分散](#25-共分散)
    - [2.6. 確率分布の例](#26-確率分布の例)
  - [第三章 情報理論](#第三章-情報理論)
    - [3.1. 自己情報量](#31-自己情報量)
    - [3.2. シャノンエントロピー](#32-シャノンエントロピー)
    - [3.3. カルバックライブラーダイバージェンス](#33-カルバックライブラーダイバージェンス)
    - [3.4. 交差エントロピー](#34-交差エントロピー)

<!-- /code_chunk_output -->

---

## 第一章 線形代数

### 1.1. スカラーとベクトル

-   **スカラー**

    -   重さなどのように「<font color=red>大きさ</font>」だけをもつ量

-   **ベクトル**

    -   風などのように「<font color=red>向き</font>」と「<font color=red>大きさ</font>」をもつ量

### 1.2. 行列

&emsp;次に述べるような数を箱に詰めたような形をしている。

$$
\begin{pmatrix}
   a & b \\
   c & d
\end{pmatrix}
$$

-   例)

$$
\begin{pmatrix}
   1 & 2 \\
   3 & 4
\end{pmatrix},
\begin{pmatrix}
    1 & 2 & 3 & 4\\
    5 & 6 & 7 & 8
\end{pmatrix},
\begin{pmatrix}
    1 & -2 & 3\\
    2 & -4 & 6\\
    3 & -6 & 9
\end{pmatrix}
$$

-   (m, n)型の行列（$m\times n$行列）

$$
A=\begin{pmatrix}
   a_{11} & a_{12} & \dots & a_{1n}\\
   a_{21} & a_{22} & \dots & a_{2n}\\
   \vdots & \vdots & & \vdots\\
   a_{m1} & a_{m2} & \dots & a_{mn}
\end{pmatrix}
$$

この行列$A$に対して

$$
\begin{pmatrix}
   a_{i1} & a_{i2} & \dots & a_{in}
\end{pmatrix}
$$

を$A$の第$i$行ベクトルといい、

$$
\begin{pmatrix}
   a_{1j} \\
   a_{2j} \\
   \vdots \\
   a_{mj} \\
\end{pmatrix}
$$

を$A$の第$j$列ベクトルという。
(m, n)型の行列$A$, $B$, $C$と実数$\lambda$, $\mu$について以下が成り立つ。

(1) $A + B = B + A$ &emsp;&emsp;&emsp;[交換法則]
(2) $A + (B + C) = (A + B) + C$ &emsp;&emsp;&emsp;[結合法則]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\lambda(\mu A)=\mu(\lambda A)$
(3) $\lambda (A + B) = \lambda A + \lambda B$ &emsp;&emsp;&emsp;[分配法則]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$(\lambda + \mu)A = \lambda A + \mu A$ &emsp;&emsp;&emsp;[分配法則]

また、(m, n)型の行列$A$、(n, p)型の行列$B$, $C$と(p, q)型の行列$D$および実数$\lambda$について以下が成り立つ。

(1) $A(BD) = (AB)D$ &emsp;&emsp;&emsp;[結合法則]
(2) $A (B + C) = AB + AC$ &emsp;&emsp;&emsp;[分配法則]
(3) $\lambda (A B) = (\lambda A)B = A(\lambda B)$

ここで、行列$A$と行列$B$の積$AB$は$(A の\color{red}{列の数}$$)=(B の\color{red}{行の数}$$)$でないと定義できないので注意する。

### 1.3. 逆行列

&emsp;数字の 1 に相当する行列として右下がりの対角線上に 1 が並ぶ行列を$n次元単位行列$という。

$$
E=\begin{pmatrix}
   1 & 0 & \dots & 0\\
   0 & 1 & \ddots & 0\\
   \vdots & \ddots & \ddots & \vdots\\
   0 & 0 & \dots & 1
\end{pmatrix}
$$

実際、(m, n)型行列$A$に対して$AE=A$が成り立つ。
$n$次元正方行列$R$に対して、$RX=XR=E_{n}$を満たす$n$次元正方行列$X$があるとき、$R$は正則であるといい、行列$X$を$R^{-1}$と表して、「$R$インバース」と読み、$R$の逆行列という。
2 次の正方行列$R$を考える。

$$
R=\begin{pmatrix}
   a & b\\
   c & d
\end{pmatrix}
$$

$ad \not = bc$であれば、$R$の逆行列$R^{-1}$は、

$$
R^{-1}=\frac{1}{ad-bc}
\begin{pmatrix}
   d & -b\\
   -c & a
\end{pmatrix}
$$

となる。
逆行列を使い、以下の連立方程式を解く。

$$
\begin{align}
   x_{1}+2x_{2}&=3 \\
   2x_{1}+5x_{2}&=5
\end{align}
$$

これを行列で表すと

$$
\begin{pmatrix}
   1 & 2\\
   2 & 5
\end{pmatrix}
\begin{pmatrix}
   x_{1}\\
   x_{2}
\end{pmatrix}=
\begin{pmatrix}
   3\\
   5
\end{pmatrix}
$$

逆行列を左からかけ

$$
\frac{1}{5-4}
\begin{pmatrix}
   5 & -2\\
   -2 & 1
\end{pmatrix}
\begin{pmatrix}
   1 & 2\\
   2 & 5
\end{pmatrix}
\begin{pmatrix}
   x_{1}\\
   x_{2}
\end{pmatrix}=
\frac{1}{5-4}
\begin{pmatrix}
   5 & -2\\
   -2 & 1
\end{pmatrix}
\begin{pmatrix}
   3\\
   5
\end{pmatrix}
$$

よって

$$
\begin{pmatrix}
    x_{1}\\
    x_{2}
\end{pmatrix}=
\begin{pmatrix}
    5\\
    -1
\end{pmatrix}
$$

### 1.4. 行列式

&emsp;行列式（Determinant）とは、行列がどのような特性を持っているかを示す数値のこと。2 次の行列式の絶対値はその成分が作るベクトルによって構成される平行四辺形の面積に等しい。3 次の場合は平行六面体の体積に一致する。行列式はスカラーになる。
2 次の正方行列の場合、

$$
A=\begin{pmatrix}
   a & b \\
   c & d
\end{pmatrix}
$$

$A$の行列式$\begin{vmatrix}
   A
\end{vmatrix}$は以下のように計算する。

$$
\begin{vmatrix}
   A
\end{vmatrix}=
ad-bc
$$

3 次の正方行列$B$の場合、行列式$\begin{vmatrix}B\end{vmatrix}$は以下のように計算する。

$$
\begin{vmatrix}B\end{vmatrix}=\begin{vmatrix}
   b_{11} & b_{12} & b_{13}\\
   b_{21} & b_{22} & b_{23}\\
   b_{31} & b_{32} & b_{33}
\end{vmatrix}\\
=b_{13}b_{21}b_{32}+b_{11}b_{22}b_{33}+b_{12}b_{23}b_{31}
\\-b_{12}b_{21}b_{33}-b_{13}b_{22}b_{31}-b_{11}b_{23}b_{32}
$$

このような計算方法をサラスの方法という。これは 4 次以上になると計算が困難になるため 3 次までしか使えない。また、別解として

$$
\begin{vmatrix}B\end{vmatrix}=\begin{vmatrix}
   b_{11} & b_{12} & b_{13}\\
   b_{21} & b_{22} & b_{23}\\
   b_{31} & b_{32} & b_{33}
\end{vmatrix}\\
=b_{11}\begin{vmatrix}
   b_{22} & b_{23}\\
   b_{32} & b_{33}
\end{vmatrix}+
b_{12}\begin{vmatrix}
   b_{21} & b_{23}\\
   b_{31} & b_{33}
\end{vmatrix}+
b_{13}\begin{vmatrix}
   b_{21} & b_{22}\\
   b_{31} & b_{32}
\end{vmatrix}
$$

が成り立つ。このように行列式の次数を下げることを余因子展開という。

### 1.5. 行基本変形

&emsp;以下に行列に関する 3 つの操作を示す。この 3 つの操作を行基本変形という。

-   (Ⅰ) 第$i$行を$\lambda(\not =0)$倍する
-   (Ⅱ) 第$i$行に第$k$行を加える
-   (Ⅲ) 第$i$行と第$k$行とを入れ替える

また、(Ⅰ)と(Ⅱ)を組み合わせた「第$i$行に第$k$行の$\lambda$倍を加える」こともできる。
行基本変形により逆行列を求める。

$$
A=\begin{pmatrix}
   1 & 2\\
   2 & 5
\end{pmatrix}
$$

$$
A^{-1}=\begin{pmatrix}
   1 & 2\vert 1 & 0\\
   2 & 5\vert 0 & 1
\end{pmatrix}\\
=\begin{pmatrix}
   1 & 2\vert 1 & 0\\
   0 & 1\vert -2 & 1
\end{pmatrix}\\
=\begin{pmatrix}
   1 & 0\vert 5 & -2\\
   0 & 1\vert -2 & 1
\end{pmatrix}
$$

計算の 1 行目->2 行目では行列の 1 行目を-2 倍したものを 2 行目に足している。
次に計算の 2 行目->3 行目では行列の 2 行目を-2 倍したものを 1 行目に足している。
ここで右側の$2\times2$の行列が逆行列となっていることがわかる。同様にして連立方程式も解ける。このような手法を掃き出し法という。

### 1.6. 固有値分解

&emsp;$n$次正方行列$A$に対して、

$$
A\upsilon = \lambda\upsilon, \quad\upsilon\not =0
$$

を満たすとき、$\upsilon$は方向を変えないベクトルとなりこれを<font color=red>固有ベクトル</font>といい、拡大・縮小率を表すスカラー$\lambda$を<font color=red>固有値</font>という。この時、

$$
(A-\lambda E)\upsilon= \textbf{0}
$$

は$\textbf{0}$以外の解を持つ。係数行列の行列式は$\begin{vmatrix} A-\lambda E \end{vmatrix}=0$である。ここである正方行列$P$を用いて、

$$
P^{-1}AP=\begin{pmatrix}
   \lambda_{1} & 0 & \dots & 0\\
   0 & \ddots & \ddots & \vdots\\
   \vdots & \ddots & \ddots & 0\\
   0 & \dots & 0 & \lambda_{n}\\
\end{pmatrix}=\varLambda
$$

と対角行列にできるとき、$A$は対角可能であるといい、

$$
A=P\varLambda P^{-1}
$$

となるように$A$を分解することを固有値分解という。$P$は固有ベクトルをの組を列ベクトルとする行列になる。固有値分解は主成分分析やサポートベクターマシンで使われる。また、行列のべき乗や指数関数などの計算が容易になることがメリットとして挙げられる。

例）

$$
A=P\varLambda P^{-1}\quad\rArr\quad A^{n}=P\varLambda^{n} P^{-1}
$$

### 1.7. 特異値分解

&emsp;固有値分解できるのは$A$が対角可能である場合に限られたが、特異値分解は任意の$(n\times m)$行列に対して適用できる。特異値分解では行列$A$を以下のように分解する。

$$
A=U\varSigma V^{T}
$$

ここで

-   $U$は$(n\times n)$の直行行列で、その列は$AA^{T}$の固有ベクトルである。
-   $\varSigma$は$(n\times m)$の対角行列で対角要素は$A$の特異値となる。また、特異値は$A$の特異値分解から得られる固有値の平行根となる。
-   $V$は$(m\times n)$の直行行列でその列は$A^{T}A$の固有ベクトルである。

特異値分解の手順

1. 行列$AA^{T}$と$A^{T}A$を計算する。
   $AA^{T}=U\varLambda U^{T}$
   $A^{T}A=V\varLambda V^{T}$
   ここで$\varLambda$は対角行列でその対角要素は$A$の特異値の 2 乗である。
2. 固有値と固有ベクトルの計算
   $U$は$AA^{T}$の固有ベクトル行列
   $V$は$A^{T}A$の固有ベクトル行列
   これらの固有ベクトルがそれぞれ左特異ベクトル、右特異ベクトルである。
3. 特異値は$\varLambda$の対角要素の平方根となる。
   $\sigma_{i}=\sqrt{\lambda_i{}}$
4. 対角行列$\varSigma$の形成
   対角行列$\varSigma$は特異値を対角要素に持つ。

## 第二章 確率・統計

### 2.1. 確率

&emsp;確率とは事象の起こりやすさを定量的に示すものであり、事象$A$の確率を probability の頭文字をとって$P(A)$で表す。試行が全部で$N$個あって、同程度の確かさで起こる事象$A$が$R$回起こったとすれば、確率は$P(A)=R/N$と定義される。これは客観的に決定されるので、客観説の立場という。これに対して、主観的にある確率を与えて主観確率を分析することを主観説の立場という。

### 2.2. 条件付確率

&emsp;事象$A$の起こる確率が他の事象$B$に影響されない場合、以下が成り立つ。

$$
P(A\cap B)=P(A)\cdot P(B)
$$

事象$B$が起こったとわかっている場合に、事象$A$の起こる確率を$B$を条件とする$A$の条件付確率と呼び、$P(A|B)$で表す。$A$の条件付確率は、$B$が起こった場合にそのうちさらに$A$が起こる確率であるから、

$$
P(A|B)=\frac{P(A\cap B)}{P(B)}
$$

と定義される。同様に

$$
P(B|A)=\frac{P(A\cap B)}{P(A)}
$$

となることから、$P(A\cap B)$を消去すると、

$$
P(B|A)=\frac{P(B)\cdot P(A|B)}{P(A)}
$$

が得られる。これをベイズの定理という。ベイズの定理は結果に対する原因の確率を計算する公式を与える。

### 2.3. 確率変数と確率分布

-   確率変数
    ランダムにある値をとる変数。$X$のように慣例的に大文字で表す。
-   確率分布
    確率変数がとりうる値それぞれの確率を表したもの。

### 2.4. 確率変数の期待値と分散

&emsp;いろいろな値を取る確率変数を代表する確率の重みつき平均を期待値$E(X)$という。

-   離散型
    $$E(X)=\sum_{x} xf(x)$$
-   連続型
    $$E(X)=\int_{-\infin}^{\infin} xf(x)dx$$

&emsp;確率分布の集中やばらつきを示す指標を分散という。分散$V(X)$は期待値$\mu$を使い以下のように定義される。

$$
V(X)=E\{(X-\mu)^2\}
$$

-   離散型
    $$V(X)=\sum_{x} (x-\mu)^2f(x)$$
-   連続型
    $$V(X)=\int_{-\infin}^{\infin} (x-\mu)^2f(x)dx$$

また、分散の定義から

$$
\begin{align}\
   \nonumber V(X)&=E\{(X-\mu)^2\} \\
   \nonumber &=E(X^2)-2\mu E(X)+\mu^2 \\
   \nonumber &=E(X^2)-\mu^2 \\
   \nonumber &=E(X^2)-(E(X))^2 \\
\end{align}
$$

となり、二乗期待値と期待値の二乗から計算できる。また、標準偏差$D(X)$は分散の平方根で表される。

$$
D(X)=\sqrt{V(X)}
$$

### 2.5. 共分散

&emsp;二つの確率変数$X, Y$の間に関連がある場合を考えると、ばらつきの指標としての分散には単純な加法が成立しない。

$$
V(X+Y)\not=V(X)+V(Y)
$$

$X+Y$のばらつきには$X,Y$単独のばらつきのほかに相互関連によるばらつき$\text{Cov}(X,Y)$が存在する。

$$
V(X+Y)=V(X)+V(Y)+2\text{Cov}(X,Y) \\
\text{Cov}(X,Y)=E\{(X-E(X))(Y-E(Y))\}
$$

$\text{Cov}(X,Y)$を$X,Y$の共分散という。共分散を$X,Y$の標準偏差で割って調整した値を相関係数という。

$$
\rho_{XY} =\frac{\text{Cov}(X,Y)}{\sqrt{V(X)}\sqrt{V(Y)}}
$$

### 2.6. 確率分布の例

-   二項分布
    成功するか失敗するか、のような 2 種類の可能な結果が生じる事象を$n$回繰り返すことをベルヌーイ試行という。成功確率$p$、失敗確率$1-p$とすると、確率分布は以下のようになる。
    $$f(x)=_nC_xp^x(1-p)^{n-x}$$

    $$
    \begin{align}\
      \nonumber E(X)&=np \\
      \nonumber V(X)&=np(1-p) \\
    \end{align}
    $$

    $n=1$の時の$f(x)$をベルヌーイ分布という。

-   ポアソン分布
    二項分布において$n$が大である一方、$p$が希少現象である場合にはポアソン分布となる。
    $$f(x)=e^{-\lambda}\frac{\lambda^x}{x!}$$

    $$
    \begin{align}\
      \nonumber E(X)&=\lambda \\
      \nonumber V(X)&=\lambda \\
    \end{align}
    $$

    これは二項分布において$np\to\lambda$となるように$n\to\infin$, $p\to0$となる極限を求めると得られる。

-   正規分布
    連続型の代表的な確率分布でガウス分布ともいう。中心極限定理と関連することから統計学において重要な分布である。
    $$f(x)=\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{1}{2\sigma^2}(x-\mu)^2)$$
    $$
    \begin{align}\
      \nonumber E(X)&=\mu \\
      \nonumber V(X)&=\sigma^2 \\
    \end{align}
    $$

## 第三章 情報理論

### 3.1. 自己情報量

&emsp;特定の出来事が発生する際の驚きの度合いを定量化する指標。ある出来事$x$が発生する確率を$P(x)$とすると、自己情報量$I(x)$は次のように定義される。

$$
I(x)=-\log_{2}P(x)
$$

対数の底が 2 の時、単位は bit となり、ネイピア数のときは nat となる。出来事の確率が低いほど自己情報量は大きくなる。これは稀な出来事が発生するほど驚きが大きいことを反映している。

### 3.2. シャノンエントロピー

&emsp;確率分布全体の不確実性や情報量を表す。ある離散型確率分布$P$のシャノンエントロピー$H(P)$は次のように定義される。

$$
H(P)=-\sum_i P(x_i)\log P(x_i)
$$

この値が高いほど不確実性や情報量が大きいことを意味する。

### 3.3. カルバックライブラーダイバージェンス

&emsp;ある確率分布$P$から別の確率分布$Q$への情報のずれや差異を測定する指標。$P$と$Q$の KL ダイバージェンス$D_{KL}(P\mid\mid Q)$は次のように定義される。

$$
D_{KL}(P\mid\mid Q)=\sum_{i}P(x_i)\log\frac{P(x_i)}{Q(x_i)}
$$

KL ダイバージェンスは非対称であり、$D_{KL}(P\mid\mid Q)\not=D_{KL}(Q\mid\mid P)$である。KL ダイバージェンスは分布$P$に基づく情報を分布$Q$で近似する際の情報損失を示す。$P$と$Q$が同じであれば KL ダイバージェンスは 0 になる。

### 3.4. 交差エントロピー

&emsp;ある確率分布$P$を使って別の確率分布$Q$を符号化する際の平均的な情報量を示す。$P$と$Q$の交差エントロピー$H(P,Q)$は次のように定義される。

$$
H(P,Q)=-\sum_i P(x_i)\log Q(x_i)
$$

交差エントロピーは分布$Q$を用いて分布$P$を表現する際の効率を示す。$Q$が$P$に近いほど交差エントロピーは小さくなる。
